{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entity linking with Wikidata (NERD/EL)"
      ],
      "metadata": {
        "id": "pte4SIhb6XBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #1. Setup development environment"
      ],
      "metadata": {
        "id": "YuOrCyv33dD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update & import Python modules"
      ],
      "metadata": {
        "id": "x5nQrrW56AvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install and download spaCy related modules\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "\n",
        "# spaCy\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import DocBin, Span\n",
        "from spacy.matcher import PhraseMatcher\n",
        "from spacy.kb import KnowledgeBase\n",
        "from spacy.training import Example\n",
        "from spacy.ml.models import load_kb\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Firebase/Firestore\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "\n",
        "# general Python modules\n",
        "import json\n",
        "import datetime\n",
        "import requests\n",
        "import csv\n",
        "import random\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "KO2xzpIn3bx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0813244-889e-4501-9aaf-f76836633d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-10 09:28:24.138808: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get access to Firebase and Drive"
      ],
      "metadata": {
        "id": "uTaktcWqxg2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remount drive, forced if needed\n",
        "drive.mount(\"/content/gdrive/\", force_remount = True)\n",
        "print(\"Stablished access to Google Drive\")\n",
        "\n",
        "# initialize Drive path\n",
        "DRIVE_PATH = \"/content/gdrive/My Drive\"\n",
        "\n",
        "# open Firebase credentials\n",
        "# with open(DRIVE_PATH + \"/ie_course/credentials/firebase_credentials.json\") as f:\n",
        "#   credential = json.load(f)\n",
        "# credential = credentials.Certificate(credential)\n",
        "\n",
        "# create Firestore database instance\n",
        "# firebase_admin.initialize_app(credential)\n",
        "# db = firestore.client()\n",
        "# print(\"Stablished access to Firestore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg1Wpiefq8DK",
        "outputId": "9a3f2b69-f61e-4109-c271-8131590ab331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "Stablished access to Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #2. Disambiguate and link NEs using a KG"
      ],
      "metadata": {
        "id": "Iqg0O5RPGDds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define input and output paths/files"
      ],
      "metadata": {
        "id": "qpeWYFZ9cfZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input files\n",
        "ents_file = DRIVE_PATH + \"/ie_course/assets/entities.csv\"\n",
        "annot_text_file = DRIVE_PATH + \"/ie_course/assets/emerson_annotated_text.jsonl\"\n",
        "\n",
        "# output files\n",
        "kb_dir = DRIVE_PATH + \"/ie_course/output/ml_el/kb\"\n",
        "nlp_dir = DRIVE_PATH + \"/ie_course/output/ml_el/my_nlp\"\n",
        "train_corpus = DRIVE_PATH + \"/ie_course/output/ml_el/train_corpus\"\n",
        "test_corpus = DRIVE_PATH + \"/ie_course/output/ml_el/test_corpus\"\n",
        "nlp_el_dir = DRIVE_PATH + \"/ie_course/output/ml_el/my_el_nlp\""
      ],
      "metadata": {
        "id": "pNo7kosMlqtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create NLP pipeline and Knowledge Graph"
      ],
      "metadata": {
        "id": "EyS_m5mMkiRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Step 1: create the Knowledge Base in NLP pipeline and write it to file \"\"\"\n",
        "\n",
        "# Helper function to read in the pre-defined entities we want to disambiguate to\n",
        "def load_entities():\n",
        "  names = dict()\n",
        "  descriptions = dict()\n",
        "  # read and iterate entities and split it into two dicts\n",
        "  with open(ents_file, newline=\"\") as f:\n",
        "    entities = csv.reader(f, delimiter=\",\")\n",
        "    # print(f\"Retrieved entities\")\n",
        "    for row in entities:\n",
        "      qid = row[0]\n",
        "      name = row[1]\n",
        "      desc = row[2]\n",
        "      names[qid] = name\n",
        "      descriptions[qid] = desc\n",
        "  # return \"names\" {id,names} and \"descriptions\" {id,descriptions}\n",
        "  return names, descriptions\n",
        "\n",
        "\n",
        "# First: create a simple model with an NER component\n",
        "# To ensure we get the correct entities for this demo, add a simple entity_ruler as well.\n",
        "nlp = spacy.load(\"en_core_web_lg\", exclude=\"parser, tagger, lemmatizer\")\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
        "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"LOWER\": \"emerson\"}]}]\n",
        "ruler.add_patterns(patterns)\n",
        "nlp.add_pipe(\"sentencizer\", first=True)\n",
        "\n",
        "name_dict, desc_dict = load_entities()\n",
        "\n",
        "kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=300)\n",
        "\n",
        "for qid, desc in desc_dict.items():\n",
        "  desc_doc = nlp(desc)\n",
        "  desc_enc = desc_doc.vector\n",
        "  # Set arbitrary value for frequency\n",
        "  kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)\n",
        "\n",
        "for qid, name in name_dict.items():\n",
        "  # set 100% prior probability P(entity|alias) for each unique name\n",
        "  kb.add_alias(alias=name, entities=[qid], probabilities=[1])\n",
        "\n",
        "qids = name_dict.keys()\n",
        "probs = [0.3 for qid in qids]\n",
        "# ensure that sum([probs]) <= 1 when setting aliases\n",
        "kb.add_alias(alias=\"Emerson\", entities=qids, probabilities=probs)  #\n",
        "\n",
        "print(f\"Entities in the KB: {kb.get_entity_strings()}\")\n",
        "print(f\"Aliases in the KB: {kb.get_alias_strings()}\")\n",
        "print()\n",
        "\n",
        "# store knowledgebase and NLP pipeline\n",
        "kb.to_disk(kb_dir)\n",
        "print(f\"Saved KB in: {kb_dir}\")\n",
        "nlp.to_disk(nlp_dir)\n",
        "print(f\"Saved NLP pipeline in: {nlp_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTADi-mmT20K",
        "outputId": "5c1bc514-1cf9-45af-c33f-2cb9a64ef402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities in the KB: ['Q215952', 'Q312545', 'Q48226']\n",
            "Aliases in the KB: ['Roy Stanley Emerson', 'Emerson Ferreira da Rosa', 'Ralph Waldo Emerson', 'Emerson']\n",
            "\n",
            "Saved KB in: /content/gdrive/My Drive/ie_course/output/ml_el/kb\n",
            "Saved NLP pipeline in: /content/gdrive/My Drive/ie_course/output/ml_el/my_nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Corpora (training and test datasets)"
      ],
      "metadata": {
        "id": "OwK6GhvS5W5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Step 2: Once we have done the manual annotations, create corpora in spaCy format. \"\"\"\n",
        "\n",
        "##############################################################\n",
        "# TODO: create annotated dataset for training before this step\n",
        "##############################################################\n",
        "\n",
        "nlp = spacy.load(nlp_dir, exclude=\"parser, tagger\")\n",
        "docs = []\n",
        "gold_ids = []\n",
        "\n",
        "with open(annot_text_file,\"r\", encoding=\"utf8\") as f:\n",
        "  for line in f:\n",
        "    example = json.loads(line)\n",
        "    sentence = example[\"text\"]\n",
        "    if example[\"answer\"] == \"accept\":\n",
        "      QID = example[\"accept\"][0]\n",
        "      doc = nlp.make_doc(sentence)\n",
        "      gold_ids.append(QID)\n",
        "      # we assume only 1 annotated span per sentence, and only 1 KB ID per span\n",
        "      entity = doc.char_span(\n",
        "        example[\"spans\"][0][\"start\"],\n",
        "        example[\"spans\"][0][\"end\"],\n",
        "        label=example[\"spans\"][0][\"label\"],\n",
        "        kb_id=QID,\n",
        "      )\n",
        "      doc.ents = [entity]\n",
        "      for i, t in enumerate(doc):\n",
        "        doc[i].is_sent_start = i == 0\n",
        "      docs.append(doc)\n",
        "\n",
        "print(\"Statistics of manually annotated data:\")\n",
        "print(Counter(gold_ids))\n",
        "print()\n",
        "\n",
        "train_docs = DocBin()\n",
        "test_docs = DocBin()\n",
        "for QID in [\"Q312545\", \"Q48226\", \"Q215952\"]:\n",
        "  indices = [i for i, j in enumerate(gold_ids) if j == QID]\n",
        "  # first 8 in training\n",
        "  for index in indices[0:8]:\n",
        "    train_docs.add(docs[index])\n",
        "  # last 2 in test\n",
        "  for index in indices[8:10]:\n",
        "    test_docs.add(docs[index])\n",
        "\n",
        "train_docs.to_disk(train_corpus)\n",
        "print(f\"Saved train corpus in: {train_corpus}\")\n",
        "test_docs.to_disk(test_corpus)\n",
        "print(f\"Saved test corpus in: {test_corpus}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrAwuYF0kSkK",
        "outputId": "bd0d98aa-8880-40a0-a866-73f69888ff8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistics of manually annotated data:\n",
            "Counter({'Q312545': 10, 'Q48226': 10, 'Q215952': 10})\n",
            "\n",
            "Saved train corpus in: /content/gdrive/My Drive/ie_course/output/ml_el/train_corpus\n",
            "Saved test corpus in: /content/gdrive/My Drive/ie_course/output/ml_el/test_corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train entity linking component (ML model)"
      ],
      "metadata": {
        "id": "MpNYXMW-5mmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Step 3: Train entity linking model. \"\"\"\n",
        "\n",
        "nlp = spacy.load(nlp_dir)\n",
        "\n",
        "TRAIN_EXAMPLES = []\n",
        "\n",
        "with open(test_corpus, \"rb\") as f:\n",
        "  doc_bin = DocBin().from_disk(test_corpus)\n",
        "  docs = doc_bin.get_docs(nlp.vocab)\n",
        "  for doc in docs:\n",
        "    TRAIN_EXAMPLES.append(Example(nlp(doc.text), doc))\n",
        "\n",
        "entity_linker = nlp.add_pipe(\"entity_linker\", config={\"incl_prior\": False}, last=True)\n",
        "entity_linker.initialize(lambda: TRAIN_EXAMPLES, nlp=nlp, kb_loader=load_kb(kb_dir))\n",
        "\n",
        "with nlp.select_pipes(enable=[\"entity_linker\"]):  # train only the entity_linker\n",
        "  optimizer = nlp.resume_training()\n",
        "  for itn in range(500):  # 500 iterations takes about a minute to train\n",
        "    random.shuffle(TRAIN_EXAMPLES)\n",
        "    batches = minibatch(TRAIN_EXAMPLES, size=compounding(4.0, 32.0, 1.001))  # increasing batch sizes\n",
        "    losses = {}\n",
        "    for batch in batches:\n",
        "      nlp.update(\n",
        "        batch,\n",
        "        drop=0.2,  # prevent overfitting\n",
        "        losses=losses,\n",
        "        sgd=optimizer,\n",
        "      )\n",
        "    if itn % 50 == 0:\n",
        "      print(itn, \"Losses\", losses)  # print the training loss\n",
        "print(itn, \"Losses\", losses)\n",
        "\n",
        "nlp.to_disk(nlp_el_dir)\n",
        "print()\n",
        "print(f\"Saved NLP pipeline in: {nlp_el_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_jMIN8Rwrxj",
        "outputId": "8adc268b-afce-4ffa-a6e8-2db9ad47ffeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Losses {'entity_linker': 1.9307233691215515}\n",
            "50 Losses {'entity_linker': 0.045570552349090576}\n",
            "100 Losses {'entity_linker': 0.022603243589401245}\n",
            "150 Losses {'entity_linker': 0.016279876232147217}\n",
            "200 Losses {'entity_linker': 0.007489040493965149}\n",
            "250 Losses {'entity_linker': 0.011767923831939697}\n",
            "300 Losses {'entity_linker': 0.005197629332542419}\n",
            "350 Losses {'entity_linker': 0.005071923136711121}\n",
            "400 Losses {'entity_linker': 0.005336344242095947}\n",
            "450 Losses {'entity_linker': 0.004006430506706238}\n",
            "499 Losses {'entity_linker': 0.0030239075422286987}\n",
            "\n",
            "Saved NLP pipeline in: /content/gdrive/My Drive/ie_course/output/ml_el/my_el_nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Entity Linking component"
      ],
      "metadata": {
        "id": "idquI3qS6Z8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Step 4: Evaluate the new Entity Linking component by applying it to unseen text. \"\"\"\n",
        "\n",
        "nlp = spacy.load(nlp_el_dir)\n",
        "\n",
        "examples = []\n",
        "\n",
        "with open(test_corpus, \"rb\") as f:\n",
        "  doc_bin = DocBin().from_disk(test_corpus)\n",
        "  docs = doc_bin.get_docs(nlp.vocab)\n",
        "  for doc in docs:\n",
        "    examples.append(Example(nlp(doc.text), doc))\n",
        "\n",
        "\n",
        "print(\"RESULTS ON THE DEV SET:\")\n",
        "print()\n",
        "\n",
        "for example in examples:\n",
        "  print(example.text)\n",
        "  print(f\"Gold annotation: {example.reference.ents[0].kb_id_}\")\n",
        "  print(f\"Predicted annotation: {example.predicted.ents[0].kb_id_}\")\n",
        "  print()\n",
        "\n",
        "print()\n",
        "print(\"RUNNING THE PIPELINE ON UNSEEN TEXT:\")\n",
        "text = \"Tennis champion Emerson was expected to win Wimbledon.\"\n",
        "doc = nlp(text)\n",
        "print(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_, ent.kb_id_)\n",
        "print()"
      ],
      "metadata": {
        "id": "fUSAHqhYkSbR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a8d650-b93f-4757-9465-d276caac3b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS ON THE DEV SET:\n",
            "\n",
            "Emerson's first Wimbledon singles title came in 1964, with a final victory over Fred Stolle.\n",
            "Gold annotation: Q312545\n",
            "Predicted annotation: Q312545\n",
            "\n",
            "Emerson was inducted into the International Tennis Hall of Fame in 1982 and the Sport Australia Hall of Fame in 1986.\n",
            "Gold annotation: Q312545\n",
            "Predicted annotation: Q312545\n",
            "\n",
            "Carlyle in particular was a strong influence on him; Emerson would later serve as an unofficial literary agent in the United States for Carlyle, and in March 1835, he tried to persuade Carlyle to come to America to lecture.\n",
            "Gold annotation: Q48226\n",
            "Predicted annotation: NIL\n",
            "\n",
            "In 1841 Emerson published Essays, his second book, which included the famous essay \"Self-Reliance\".\n",
            "Gold annotation: Q48226\n",
            "Predicted annotation: NIL\n",
            "\n",
            "Emerson scored his second international goal on 31 March 1999, in a friendly match against Japan in Tokyo, which Brazil won 2-0.\n",
            "Gold annotation: Q215952\n",
            "Predicted annotation: Q215952\n",
            "\n",
            "Emerson made his Brazil debut on 10 September 1997, in a home friendly match against Ecuador, in Salvador, Bahia, also scoring a goal in the match, as Brazil went on to win 4-2.\n",
            "Gold annotation: Q215952\n",
            "Predicted annotation: Q215952\n",
            "\n",
            "\n",
            "RUNNING THE PIPELINE ON UNSEEN TEXT:\n",
            "Tennis champion Emerson was expected to win Wimbledon.\n",
            "Emerson PERSON Q312545\n",
            "Wimbledon DATE NIL\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}