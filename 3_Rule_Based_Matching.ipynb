{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOmN99dHBR3"
      },
      "source": [
        "# Rule-Based Matching for RE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOSa3mYvK5yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c2f042-2c0b-4998-c78e-22c2600c9435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-18 14:20:58.736326: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-18 14:21:00.614870: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-18 14:21:00.615000: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-18 14:21:00.615024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (63.4.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.matcher import DependencyMatcher\n",
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "cuGVbyBNMHTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "metadata": {
        "id": "0Ugs5fg0MQg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"United States works on legislation.\"\n",
        "entities = [\"United States\", \"legislation\"]\n",
        "relation = \"work\""
      ],
      "metadata": {
        "id": "BxgbhvXGLnE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sentence)"
      ],
      "metadata": {
        "id": "wQITK9ZaOOSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "VERBS = ['ROOT', 'advcl']\n",
        "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "\n",
        "object_pronouns = [\"me\", \"us\", \"them\", \"you\", \"him\", \"her\", \"it\"]\n",
        "subject_pronouns = [\"i\", \"we\", \"they\", \"you\", \"he\", \"she\", \"it\"]\n",
        "\n",
        "subject_ls = []\n",
        "object_ls = []\n",
        "\n",
        "for token in doc:\n",
        "  if token.dep_ in SUBJECTS and str(token.lower_) not in subject_pronouns:\n",
        "    subject_ls.append(token.text)\n",
        "  elif token.dep_ in OBJECTS and str(token.lower_) not in object_pronouns:\n",
        "    object_ls.append(token.text)\n",
        "\n",
        "print(subject_ls)\n",
        "print(object_ls)\n",
        "\n",
        "subjects = []\n",
        "objects = []\n",
        "\n",
        "for s in subject_ls:\n",
        "  for e in entities:\n",
        "    if s in e:\n",
        "      subjects.append(e)\n",
        "\n",
        "for o in object_ls:\n",
        "  for e in entities:\n",
        "    if o in e:\n",
        "      objects.append(e)\n",
        "\n",
        "print()\n",
        "print(subjects)\n",
        "print(objects)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iqWJNfjcwKR",
        "outputId": "91aca66d-9221-4465-c47d-727a5b5ecf32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['States']\n",
            "['legislation']\n",
            "\n",
            "['United States']\n",
            "['legislation']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_span_pattern(entity):\n",
        "  doc = nlp(entity)\n",
        "  if len(doc) == 1:\n",
        "    return {\"TEXT\": doc[0].text}\n",
        "  elif len(doc) == 2:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text}\n",
        "  elif len(doc) == 3:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text}\n",
        "  elif len(doc) == 4:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text, \"TEXT\": doc[3].text}\n",
        "  elif len(doc) == 5:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text, \"TEXT\": doc[3].text, \"TEXT\": doc[4].text}\n",
        "  elif len(doc) == 6:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text, \"TEXT\": doc[3].text, \"TEXT\": doc[4].text, \"TEXT\": doc[5].text}\n",
        "  elif len(doc) == 7:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text, \"TEXT\": doc[3].text, \"TEXT\": doc[4].text, \"TEXT\": doc[5].text, \"TEXT\": doc[6].text}\n",
        "  elif len(doc) == 8:\n",
        "    return {\"TEXT\": doc[0].text, \"TEXT\": doc[1].text, \"TEXT\": doc[2].text, \"TEXT\": doc[3].text, \"TEXT\": doc[4].text, \"TEXT\": doc[5].text, \"TEXT\": doc[6].text, \"TEXT\": doc[7].text}"
      ],
      "metadata": {
        "id": "bNANYviaiVGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "1HzYfA4QOJ20",
        "outputId": "87266c67-c088-4aaa-86f5-65a2f17b54e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"fe1b5df4357841d6b423c14e5bacfa43-0\" class=\"displacy\" width=\"925\" height=\"224.5\" direction=\"ltr\" style=\"max-width: none; height: 224.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">United</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">States</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">works</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">on</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"134.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">legislation.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fe1b5df4357841d6b423c14e5bacfa43-0-0\" stroke-width=\"2px\" d=\"M70,89.5 C70,2.0 225.0,2.0 225.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fe1b5df4357841d6b423c14e5bacfa43-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,91.5 L62,79.5 78,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fe1b5df4357841d6b423c14e5bacfa43-0-1\" stroke-width=\"2px\" d=\"M245,89.5 C245,2.0 400.0,2.0 400.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fe1b5df4357841d6b423c14e5bacfa43-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,91.5 L237,79.5 253,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fe1b5df4357841d6b423c14e5bacfa43-0-2\" stroke-width=\"2px\" d=\"M420,89.5 C420,2.0 575.0,2.0 575.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fe1b5df4357841d6b423c14e5bacfa43-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M575.0,91.5 L583.0,79.5 567.0,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-fe1b5df4357841d6b423c14e5bacfa43-0-3\" stroke-width=\"2px\" d=\"M595,89.5 C595,2.0 750.0,2.0 750.0,89.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-fe1b5df4357841d6b423c14e5bacfa43-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,91.5 L758.0,79.5 742.0,79.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dependency and phrase matchers\n",
        "dep_matcher = DependencyMatcher(nlp.vocab, validate=True)\n",
        "\n",
        "pattern = [\n",
        "    {\n",
        "      \"RIGHT_ID\": \"anchor_relation\",\n",
        "      \"RIGHT_ATTRS\": {\"TEXT\": \"works\"}\n",
        "    },\n",
        "    {\n",
        "      \"LEFT_ID\": \"anchor_relation\",\n",
        "      \"REL_OP\": \">>\",\n",
        "      \"RIGHT_ID\": \"subject\",\n",
        "      \"RIGHT_ATTRS\": create_span_pattern(\"United States\")\n",
        "    },\n",
        "    {\n",
        "      \"LEFT_ID\": \"anchor_relation\",\n",
        "      \"REL_OP\": \">>\",\n",
        "      \"RIGHT_ID\": \"object\",\n",
        "      \"RIGHT_ATTRS\": create_span_pattern(\"legislation\")\n",
        "    }\n",
        "  ]\n",
        "\n",
        "# add pattern to extract SPO triples using the DEP matcher\n",
        "dep_matcher.add(\"semantic_triple\", [pattern])"
      ],
      "metadata": {
        "id": "vuSfttdNMbt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract matches of SPO triples\n",
        "pso_matches = dep_matcher(doc)\n",
        "\n",
        "pprint(pso_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbdcygb0T5tK",
        "outputId": "b085011b-a9c0-44de-e95b-45b59f3d4409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(4699203773119030710, [2, 1, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "314nLwgoXics"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import hashlib\n",
        "import spacy\n",
        "import os\n",
        "from spacy.matcher import DependencyMatcher\n",
        "from spacy.matcher import PhraseMatcher\n",
        "#from models.statements import create_statement\n",
        "# from pprint import pprint\n",
        "\n",
        "\n",
        "def process_extract_triples():\n",
        "  \"\"\"###################################\n",
        "  # Load/initialize main data structures and spaCy NLP pipeline\n",
        "  ###################################\"\"\"\n",
        "\n",
        "  SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
        "  VERBS = ['ROOT', 'advcl']\n",
        "  OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\", 'pobj']\n",
        "\n",
        "  object_pronouns = [\"me\", \"us\", \"them\", \"you\", \"him\", \"her\", \"it\"]\n",
        "  subject_pronouns = [\"i\", \"we\", \"they\", \"you\", \"he\", \"she\", \"it\"]\n",
        "\n",
        "  # load relations (a list)\n",
        "  with open(\"shared_data/relations.json\", encoding=\"utf8\") as file:\n",
        "    relations_dict = json.load(file)\n",
        "  # load dictionary of relations/gazetteers (a list)\n",
        "  with open(\"shared_data/relations_dictionary.json\", encoding=\"utf8\") as file:\n",
        "    relations_dictionary = json.load(file)\n",
        "  # load issues/entities (a list)\n",
        "  with open(\"shared_data/issues_taxonomy.json\", encoding=\"utf8\") as file:\n",
        "    entities_taxonomy = json.load(file)\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "  rel_matcher = PhraseMatcher(nlp.vocab, None)\n",
        "\n",
        "  # create pattern of phrase matcher for relations using gazetteers\n",
        "  rel_patterns = [nlp.make_doc(gazetteer) for gazetteer in relations_dictionary]\n",
        "  rel_matcher.add(\"relations\", rel_patterns)\n",
        "\n",
        "\n",
        "  def create_span_pattern(entity):\n",
        "    _doc = nlp(entity)\n",
        "    if len(_doc) == 1:\n",
        "      return {\"TEXT\": _doc[0].text}\n",
        "    elif len(_doc) == 2:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text}\n",
        "    elif len(_doc) == 3:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text}\n",
        "    elif len(_doc) == 4:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text, \"TEXT\": _doc[3].text}\n",
        "    elif len(_doc) == 5:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text, \"TEXT\": _doc[3].text,\n",
        "              \"TEXT\": _doc[4].text}\n",
        "    elif len(_doc) == 6:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text, \"TEXT\": _doc[3].text,\n",
        "              \"TEXT\": _doc[4].text,\n",
        "              \"TEXT\": _doc[5].text}\n",
        "    elif len(_doc) == 7:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text, \"TEXT\": _doc[3].text,\n",
        "              \"TEXT\": _doc[4].text,\n",
        "              \"TEXT\": _doc[5].text, \"TEXT\": _doc[6].text}\n",
        "    elif len(_doc) == 8:\n",
        "      return {\"TEXT\": _doc[0].text, \"TEXT\": _doc[1].text, \"TEXT\": _doc[2].text, \"TEXT\": _doc[3].text,\n",
        "              \"TEXT\": _doc[4].text,\n",
        "              \"TEXT\": _doc[5].text, \"TEXT\": _doc[6].text, \"TEXT\": _doc[7].text}\n",
        "\n",
        "  # SPO triple function\n",
        "  def extract_triples_from_sentence(_sent):\n",
        "\n",
        "    # initial process of the sentence with the NLP pipeline\n",
        "    doc = nlp(_sent[\"sentence\"])\n",
        "\n",
        "    #################################################\n",
        "    # 1. use phrase matcher to identify occurrences of predicates and entities\n",
        "    #################################################\n",
        "\n",
        "    # matches for relation (1 or many extractions)\n",
        "    rel_matches = rel_matcher(doc)\n",
        "    # get relation spans from relation matches\n",
        "    rel_spans = [doc[start:end] for _, start, end in rel_matches]\n",
        "    # filter overlapping spans to ensure uniqueness of relations\n",
        "    relation_spans = spacy.util.filter_spans(rel_spans)\n",
        "    # create list of extracted relations in string type\n",
        "    relations = [r.lemma_ for r in relation_spans]\n",
        "\n",
        "    #################################################\n",
        "    # 2. use relations (predicates) occurrence in sentence to identify entities\n",
        "    #################################################\n",
        "\n",
        "    # container of extracted triples. even though it may be unlikely,\n",
        "    # a single sentence may contain more than 1 triple\n",
        "    TRIPLES = []\n",
        "\n",
        "    entities = _sent[\"entities\"]\n",
        "    # entities_list = [e[3] for e in entities]\n",
        "\n",
        "    if relations and entities:\n",
        "      # print(relations)\n",
        "\n",
        "      subject_ls = []\n",
        "      object_ls = []\n",
        "\n",
        "      for token in doc:\n",
        "        if token.dep_ in SUBJECTS and str(token.lower_) not in subject_pronouns:\n",
        "          subject_ls.append([token.text, token.idx, token.idx + len(token.text) - 1])\n",
        "        elif token.dep_ in OBJECTS and str(token.lower_) not in object_pronouns:\n",
        "          object_ls.append([token.text, token.idx, token.idx + len(token.text) - 1])\n",
        "\n",
        "      subjects = []\n",
        "      objects = []\n",
        "\n",
        "      for s in subject_ls:\n",
        "        for e in entities:\n",
        "          _range = range(e[0], e[1], 1)\n",
        "          if s[1] in _range and s[2] in _range:\n",
        "            subjects.append(e)\n",
        "\n",
        "      for o in object_ls:\n",
        "        for e in entities:\n",
        "          _range = range(e[0], e[1], 1)\n",
        "          if o[1] in _range and o[2] in _range:\n",
        "            objects.append(e)\n",
        "\n",
        "      if subjects and objects:\n",
        "\n",
        "        pso_matches = []\n",
        "        for r in relations:\n",
        "          for s in subjects:\n",
        "            for o in objects:\n",
        "              # create dependency and phrase matchers\n",
        "              dep_matcher = DependencyMatcher(nlp.vocab, validate=True)\n",
        "\n",
        "              pattern = [\n",
        "                {\n",
        "                  \"RIGHT_ID\": \"anchor_relation\",\n",
        "                  \"RIGHT_ATTRS\": {\"LEMMA\": r}\n",
        "                },\n",
        "                {\n",
        "                  \"LEFT_ID\": \"anchor_relation\",\n",
        "                  \"REL_OP\": \">\",\n",
        "                  \"RIGHT_ID\": \"subject\",\n",
        "                  \"RIGHT_ATTRS\": create_span_pattern(s[3])\n",
        "                },\n",
        "                {\n",
        "                  \"LEFT_ID\": \"anchor_relation\",\n",
        "                  \"REL_OP\": \">\",\n",
        "                  \"RIGHT_ID\": \"object\",\n",
        "                  \"RIGHT_ATTRS\": create_span_pattern(o[3])\n",
        "                }\n",
        "              ]\n",
        "\n",
        "              # add pattern to extract SPO triples using the DEP matcher\n",
        "              dep_matcher.add(\"semantic_triple\", [pattern])\n",
        "              # extract matches of SPO triples\n",
        "              pso_matches1 = dep_matcher(doc)\n",
        "\n",
        "              if pso_matches1:\n",
        "                for m in pso_matches1:\n",
        "                  pso_matches.append(m)\n",
        "\n",
        "        if pso_matches:\n",
        "          for pso_match in pso_matches:\n",
        "            obj_token = doc[pso_match[1][2]]\n",
        "            obj_match = [obj_token.idx, obj_token.idx + len(obj_token.text) - 1]\n",
        "            # print(obj_match)\n",
        "            rel_match = doc[pso_match[1][0]].text\n",
        "            subj_token = doc[pso_match[1][1]]\n",
        "            subj_match = [subj_token.idx, subj_token.idx + len(subj_token.text) - 1]\n",
        "\n",
        "            _object = None\n",
        "            _subject = None\n",
        "\n",
        "            for o in objects:\n",
        "              _range = range(o[0], o[1], 1)\n",
        "              if obj_match[0] in _range and obj_match[1] in _range:\n",
        "                _object = o[3]\n",
        "\n",
        "            for s in subjects:\n",
        "              _range = range(s[0], s[1], 1)\n",
        "              if subj_match[0] in _range and subj_match[1] in _range:\n",
        "                _subject = s[3]\n",
        "\n",
        "            if _object and _subject and _object != _subject:\n",
        "              obj_chunk = None\n",
        "              subj_chunk = None\n",
        "              for noun_chunk in doc.noun_chunks:\n",
        "\n",
        "                if obj_token in noun_chunk and _object in noun_chunk.text:\n",
        "                  if noun_chunk:\n",
        "                    obj_chunk = noun_chunk.text\n",
        "                  else:\n",
        "                    obj_chunk = \"\"\n",
        "                if subj_token in noun_chunk and _subject in noun_chunk.text:\n",
        "                  if noun_chunk:\n",
        "                    subj_chunk = noun_chunk.text\n",
        "                  else:\n",
        "                    subj_chunk = \"\"\n",
        "\n",
        "              _triple = [\n",
        "                (look_up_wikidata_id(\"entity\", _subject), _subject, subj_chunk),\n",
        "                (look_up_wikidata_id(\"relation\", rel_match), rel_match),\n",
        "                (look_up_wikidata_id(\"entity\", _object), _object, obj_chunk)\n",
        "              ]\n",
        "\n",
        "              if _triple not in TRIPLES:\n",
        "                TRIPLES.append(_triple)\n",
        "\n",
        "    return TRIPLES"
      ],
      "metadata": {
        "id": "jKZMRuv-BsdR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}