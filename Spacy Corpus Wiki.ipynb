{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic corpus building using Wikipedia"
      ],
      "metadata": {
        "id": "pte4SIhb6XBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #1. Setup development environment"
      ],
      "metadata": {
        "id": "YuOrCyv33dD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update & import Python modules"
      ],
      "metadata": {
        "id": "x5nQrrW56AvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install and download spaCy related modules\n",
        "!pip install --upgrade spacy\n",
        "!python -m spacy download en_core_web_lg # using small model (sm)\n",
        "!pip install wikipedia\n",
        "!pip install bs4\n",
        "\n",
        "# spaCy\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Span\n",
        "from spacy.matcher import PhraseMatcher\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Firebase/Firestore\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials\n",
        "from firebase_admin import firestore\n",
        "\n",
        "# Beautiful Soup\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Wikipedia API\n",
        "import wikipedia\n",
        "\n",
        "# general Python modules\n",
        "import json\n",
        "import datetime\n",
        "import requests\n",
        "from pprint import pprint\n",
        "import re"
      ],
      "metadata": {
        "id": "KO2xzpIn3bx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get access to Firebase and Drive"
      ],
      "metadata": {
        "id": "uTaktcWqxg2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remount drive, forced if needed\n",
        "drive.mount(\"/content/gdrive/\", force_remount = True)\n",
        "print(\"Stablished access to Google Drive\")\n",
        "\n",
        "# initialize Drive path\n",
        "DRIVE_PATH = \"/content/gdrive/My Drive\"\n",
        "\n",
        "# open Firebase credentials\n",
        "with open(DRIVE_PATH + \"/ie_course/credentials/firebase_credentials.json\") as f:\n",
        "  credential = json.load(f)\n",
        "credential = credentials.Certificate(credential)\n",
        "\n",
        "# create Firestore database instance\n",
        "firebase_admin.initialize_app(credential)\n",
        "db = firestore.client()\n",
        "print(\"Stablished access to Firestore\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg1Wpiefq8DK",
        "outputId": "b665da8d-f8c3-4e67-98e8-4ce7ea1e3a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "Stablished access to Google Drive\n",
            "Stablished access to Firestore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #2. Build corpus from Wikipedia"
      ],
      "metadata": {
        "id": "8n4pgjalsusZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 1: Scrape text from Wikipedia article"
      ],
      "metadata": {
        "id": "WLQT-5dF98F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wikipedia slug of entity\n",
        "#entity_slug = \"Kamala_Harris\"\n",
        "entity_slug = \"Joe_Biden\"\n",
        "\n",
        "# parse text from a Wikipedia page, from p elements\n",
        "r = requests.get(f\"https://en.wikipedia.org/wiki/{entity_slug}\")\n",
        "soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "p_els = soup.find_all(\"p\")\n",
        "text = [p.text for p in p_els]\n",
        "\n",
        "# basic text preprocessing\n",
        "processed_text = []\n",
        "for p in text:\n",
        "  p = p.replace(\"\\n\", \"\") # remove new line chars\n",
        "  p = p.lstrip() # remove leading blank spaces\n",
        "  p = p.rstrip() # remove trailing blank space\n",
        "  if p == \"\": # ignore empty paragraphs\n",
        "    continue\n",
        "  # remove citation numbers [x]\n",
        "  regex_wikipedia_citation = \"(\\[\\d+(,\\s?\\d+|\\d*-\\d+)*\\])\"\n",
        "  loops = 0\n",
        "  while loops < 6:\n",
        "    loops += 1\n",
        "    match = re.search(regex_wikipedia_citation, p)\n",
        "    if match:\n",
        "      string = match.group()\n",
        "      p = p.replace(string, \"\")\n",
        "\n",
        "  processed_text.append(p)\n",
        "text = processed_text\n",
        "\n",
        "# initialize spaCY pipeline and container of sentences\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "sentences_container = []\n",
        "\n",
        "# split text into sentences\n",
        "for index, paragraph in enumerate(text):\n",
        "  # split paragraph in sentences\n",
        "  sentences = [sent.text for sent in nlp(paragraph).sents]\n",
        "  sentences_container.extend(sentences)\n",
        "\n",
        "# save record in JSON file\n",
        "with open(DRIVE_PATH + f\"/ie_course/output/{entity_slug.lower()}_context_texts_1.json\", \"w\", encoding = \"utf-8\") as f:\n",
        "  json.dump(sentences_container, f, ensure_ascii = False, indent = 2)\n",
        "  print(f\"Saved {len(sentences_container)} context sentences\")"
      ],
      "metadata": {
        "id": "b_365mkOjtSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e60fdd-555d-4624-e3aa-4ab05dd365dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 459 context sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method 2: Retrieve Wikipedia API (Python module)\n",
        "Review the official Wikipedia API https://pypi.org/project/wikipedia/"
      ],
      "metadata": {
        "id": "Mxt5eCZdK12W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity_name = \"Joe Biden\"\n",
        "\n",
        "# parse text content from Wikipedia article\n",
        "wikipedia_page = wikipedia.page(entity_name, auto_suggest=False)\n",
        "text = wikipedia_page.content\n",
        "\n",
        "# initialize spaCY pipeline and container of sentences\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "sentences_container = []\n",
        "\n",
        "# split text into sentences\n",
        "sentences = [sent.text for sent in nlp(text).sents]\n",
        "\n",
        "# basic text preprocessing\n",
        "processed_text = []\n",
        "for sent in sentences:\n",
        "  sent = sent.replace(\"\\n\", \"\") # remove new line chars\n",
        "  sent = sent.lstrip() # remove leading blank spaces\n",
        "  sent = sent.rstrip() # remove trailing blank space\n",
        "  if sent == \"\": # ignore empty sentences\n",
        "    continue\n",
        "  # remove citation numbers [x]\n",
        "  regex_wikipedia_citation = \"(\\[\\d+(,\\s?\\d+|\\d*-\\d+)*\\])\"\n",
        "  loops = 0\n",
        "  while loops < 6:\n",
        "    loops += 1\n",
        "    match = re.search(regex_wikipedia_citation, sent)\n",
        "    if match:\n",
        "      string = match.group()\n",
        "      sent = sent.replace(string, \"\")\n",
        "\n",
        "  processed_text.append(sent)\n",
        "text = processed_text\n",
        "\n",
        "# save record in JSON file\n",
        "with open(DRIVE_PATH + f\"/ie_course/output/{entity_slug.lower()}_context_texts_2.json\", \"w\", encoding = \"utf-8\") as f:\n",
        "  json.dump(text, f, ensure_ascii = False, indent = 2)\n",
        "  print(f\"Saved {len(text)} context sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtaVe4fOPX4N",
        "outputId": "e7f0e66e-9530-4777-df1b-7402b4b2fbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 445 context sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create lexicon of entity name/aliases from Wikidata"
      ],
      "metadata": {
        "id": "wDbOv8seq3Rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Retrieve entity info from Wikidata and make a list of aliases, by combining label + aliases \"\"\"\n",
        "\n",
        "# Wikidata id of entity\n",
        "#qid = \"Q10853588\" # Kamala Harris\n",
        "qid = \"Q6279\" # Joe Biden\n",
        "\n",
        "# fetch entity info from the Wikidata API (entity endpoint)\n",
        "api_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
        "r = requests.get(api_url, params={\"format\": \"json\"})\n",
        "# simplify access to root elements of JSON object\n",
        "entity_info = r.json()[\"entities\"][f\"{qid}\"]\n",
        "\n",
        "# get entity aliases\n",
        "if entity_info[\"aliases\"].get(\"en\"):\n",
        "  aliases = [a[\"value\"] for a in entity_info[\"aliases\"][\"en\"]] if entity_info[\"aliases\"].get(\"en\") else []\n",
        "\n",
        "# create container of gazetteers\n",
        "gazetteers = aliases\n",
        "\n",
        "# get entity name\n",
        "if entity_info[\"labels\"].get(\"en\"):\n",
        "  gazetteers.append(entity_info[\"labels\"][\"en\"][\"value\"])\n",
        "\n",
        "# get last name\n",
        "last_name = entity_info[\"labels\"][\"en\"][\"value\"].split()[-1]\n",
        "gazetteers.append(last_name)\n",
        "\n",
        "\n",
        "pprint(gazetteers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkbSX5ainytg",
        "outputId": "d74cbe9b-beb3-4a39-ea58-59b72e35e040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Joseph Biden',\n",
            " 'Joseph R. Biden',\n",
            " 'Joseph R. Biden Jr.',\n",
            " 'Joseph R. Biden, Jr.',\n",
            " 'Biden',\n",
            " 'Joey Biden',\n",
            " 'JRB',\n",
            " 'POTUS 46',\n",
            " 'Joe R. Biden Jr.',\n",
            " 'Joseph Robinette Biden',\n",
            " 'President Biden',\n",
            " 'President Joe Biden',\n",
            " 'President Joseph Biden',\n",
            " 'President Joseph R. Biden',\n",
            " 'Joseph Robinette Biden Jr.',\n",
            " 'President Joseph Biden Jr.',\n",
            " 'President Joseph Robinette Biden',\n",
            " 'President Joseph R. Biden Jr.',\n",
            " 'Joe R. Biden',\n",
            " 'President Joseph Robinette Biden Jr.',\n",
            " 'Joe Biden Jr.',\n",
            " 'Dark Brandon',\n",
            " 'Joe Biden',\n",
            " 'Biden']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create NLP pipeline and add PhraseMatcher"
      ],
      "metadata": {
        "id": "T8bLkFHD-31p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize spaCY phrase matcher (rule-based)\n",
        "matcher = PhraseMatcher(nlp.vocab, None)\n",
        "# load issues as gazetteers\n",
        "patterns = [nlp.make_doc(g) for g in gazetteers]\n",
        "matcher.add(\"gazetteers\", patterns)"
      ],
      "metadata": {
        "id": "vOXHQWX1jtPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use gazetteers to filter contextual sentences"
      ],
      "metadata": {
        "id": "O74U3r6ABN7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with open(DRIVE_PATH + f\"/ie_course/output/{entity_slug.lower()}_context_texts_1.json\") as f:\n",
        "with open(DRIVE_PATH + f\"/ie_course/output/{entity_slug.lower()}_context_texts_2.json\") as f:\n",
        "  text = json.load(f)\n",
        "\n",
        "main_text_container = []\n",
        "\n",
        "for index, paragraph in enumerate(text):\n",
        "  # split paragraph in sentences\n",
        "  sentences = [sent.text for sent in nlp(paragraph).sents]\n",
        "\n",
        "  # instance a pipeline to process sentences individually\n",
        "  disabled_pipelines = [\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"]\n",
        "  for doc in nlp.pipe(sentences, batch_size=50, disable=disabled_pipelines):\n",
        "    sent = doc.text  # sentence\n",
        "\n",
        "    # identify gazetteer contained in Doc object (text)\n",
        "    gazetteers = matcher(doc)\n",
        "    # convert gazetteers as spans\n",
        "    gazetteers = [doc[start:end] for _, start, end in gazetteers]\n",
        "    # filter overlaping matches (spans) - keep gazetteers uniqueness\n",
        "    filtered_matches = spacy.util.filter_spans(gazetteers)\n",
        "\n",
        "    # filter sentences with gazetteers occurrences\n",
        "    sentence_data = []\n",
        "    if len(filtered_matches):\n",
        "      sentence_data.append(sent)\n",
        "      entities = []\n",
        "      for m in filtered_matches:\n",
        "        span = doc[m.start:m.end]  # identify span\n",
        "        matched_gazetteer = span.text\n",
        "        match_info = (span.start_char, span.end_char, \"PERSON\")\n",
        "        entities.append(match_info)\n",
        "      sentence_data.append({\"entities\": entities})\n",
        "      main_text_container.append(sentence_data)\n",
        "\n",
        "# save record in JSON file\n",
        "with open(DRIVE_PATH + f\"/ie_course/output/{entity_slug.lower()}_ner_corpus.json\", \"w\", encoding = \"utf-8\") as f:\n",
        "  json.dump(main_text_container, f, ensure_ascii = False, indent = 2)\n",
        "  print(f\"Saved {len(main_text_container)} annotated sentences\")"
      ],
      "metadata": {
        "id": "U12UoBPMjtMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a79593-5c02-4238-b577-ec99e08e315f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 277 annotated sentences\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}